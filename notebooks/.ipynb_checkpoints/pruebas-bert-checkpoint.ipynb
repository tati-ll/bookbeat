{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones:\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba modelo BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas con modelo BERT 'sentence-transformers/msmarco-MiniLM-L-12-v3'.\n",
    "\n",
    "Este modelo crea embeddings de los textos y luego se comparan los embeddings para encontrar la similitud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para pre-procesado ya creadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocess(sentence):\n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_techniques(sentence): \n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "    \n",
    "    #stopwords â€“ no recomendado para sentiment analysis\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stopwords_removed = [w for w in tokenized_sentence if not w in stop_words]\n",
    "    \n",
    "    # Lemmatizing the verbs\n",
    "    verb_lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in stopwords_removed\n",
    "    ]\n",
    "    \n",
    "    # 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [                 \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "        for word in verb_lemmatized\n",
    "     ]\n",
    "\n",
    "    cleaned_sentence = ' '.join(word for word in noun_lemmatized)\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_techniques_2(sentence):\n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "    \n",
    "    # Lemmatizing the verbs\n",
    "    verb_lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in tokenized_sentence\n",
    "    ]\n",
    "    \n",
    "    # 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [                 \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "        for word in verb_lemmatized\n",
    "     ]\n",
    "\n",
    "    cleaned_sentence = ' '.join(word for word in noun_lemmatized)\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = pd.read_csv('../raw_data/books_with_blurbs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean book's blurs\n",
    "book_df['base_cleaned_blur'] = book_df['Blurb'].apply(basic_preprocess)\n",
    "book_df['full_preprocess_blur'] = book_df['base_cleaned_blur'].apply(preprocessing_techniques)\n",
    "book_df['preprocess_with_stopw'] = book_df['base_cleaned_blur'].apply(preprocessing_techniques_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Blurb</th>\n",
       "      <th>base_cleaned_blur</th>\n",
       "      <th>full_preprocess_blur</th>\n",
       "      <th>preprocess_with_stopw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>Here, for the first time in paperback, is an o...</td>\n",
       "      <td>here for the first time in paperback is an out...</td>\n",
       "      <td>first time paperback outstanding military hist...</td>\n",
       "      <td>here for the first time in paperback be an out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>The fascinating, true story of the world's dea...</td>\n",
       "      <td>the fascinating true story of the worlds deadl...</td>\n",
       "      <td>fascinate true story world deadliest disease g...</td>\n",
       "      <td>the fascinate true story of the world deadlies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0399135782</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>Winnie and Helen have kept each others worst s...</td>\n",
       "      <td>winnie and helen have kept each others worst s...</td>\n",
       "      <td>winnie helen keep others worst secret fifty ye...</td>\n",
       "      <td>winnie and helen have keep each others worst s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                              Title  \\\n",
       "0  0060973129                               Decision in Normandy   \n",
       "1  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "2  0399135782                             The Kitchen God's Wife   \n",
       "\n",
       "             Author  Year             Publisher  \\\n",
       "0      Carlo D'Este  1991       HarperPerennial   \n",
       "1  Gina Bari Kolata  1999  Farrar Straus Giroux   \n",
       "2           Amy Tan  1991      Putnam Pub Group   \n",
       "\n",
       "                                               Blurb  \\\n",
       "0  Here, for the first time in paperback, is an o...   \n",
       "1  The fascinating, true story of the world's dea...   \n",
       "2  Winnie and Helen have kept each others worst s...   \n",
       "\n",
       "                                   base_cleaned_blur  \\\n",
       "0  here for the first time in paperback is an out...   \n",
       "1  the fascinating true story of the worlds deadl...   \n",
       "2  winnie and helen have kept each others worst s...   \n",
       "\n",
       "                                full_preprocess_blur  \\\n",
       "0  first time paperback outstanding military hist...   \n",
       "1  fascinate true story world deadliest disease g...   \n",
       "2  winnie helen keep others worst secret fifty ye...   \n",
       "\n",
       "                               preprocess_with_stopw  \n",
       "0  here for the first time in paperback be an out...  \n",
       "1  the fascinate true story of the world deadlies...  \n",
       "2  winnie and helen have keep each others worst s...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df.to_csv('books_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df = pd.read_csv('../raw_data/spotify_millsongdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean songs's text\n",
    "songs_df['base_cleaned_text'] = songs_df['text'].apply(basic_preprocess)\n",
    "songs_df['full_preprocess_text'] = songs_df['base_cleaned_text'].apply(preprocessing_techniques)\n",
    "songs_df['preprocess_with_stopw'] = songs_df['base_cleaned_text'].apply(preprocessing_techniques_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>base_cleaned_text</th>\n",
       "      <th>full_preprocess_text</th>\n",
       "      <th>preprocess_with_stopw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
       "      <td>look at her face its a wonderful face  \\r\\nand...</td>\n",
       "      <td>look face wonderful face mean something specia...</td>\n",
       "      <td>look at her face it a wonderful face and it me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "      <td>take it easy with me please  \\r\\ntouch me gent...</td>\n",
       "      <td>take easy please touch gently like summer even...</td>\n",
       "      <td>take it easy with me please touch me gently li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
       "      <td>ill never know why i had to go  \\r\\nwhy i had ...</td>\n",
       "      <td>ill never know go put lousy rotten show boy to...</td>\n",
       "      <td>ill never know why i have to go why i have to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "\n",
       "                                                text  \\\n",
       "0  Look at her face, it's a wonderful face  \\r\\nA...   \n",
       "1  Take it easy with me, please  \\r\\nTouch me gen...   \n",
       "2  I'll never know why I had to go  \\r\\nWhy I had...   \n",
       "\n",
       "                                   base_cleaned_text  \\\n",
       "0  look at her face its a wonderful face  \\r\\nand...   \n",
       "1  take it easy with me please  \\r\\ntouch me gent...   \n",
       "2  ill never know why i had to go  \\r\\nwhy i had ...   \n",
       "\n",
       "                                full_preprocess_text  \\\n",
       "0  look face wonderful face mean something specia...   \n",
       "1  take easy please touch gently like summer even...   \n",
       "2  ill never know go put lousy rotten show boy to...   \n",
       "\n",
       "                               preprocess_with_stopw  \n",
       "0  look at her face it a wonderful face and it me...  \n",
       "1  take it easy with me please touch me gently li...  \n",
       "2  ill never know why i have to go why i have to ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df.to_csv('songs_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traer el modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder usar el modelo, primero se debe instalar:\n",
    "\n",
    "- \"pip install sentence-transformers\"\n",
    "- \"pip install torch\"\n",
    "\n",
    "Luego traermos el modelo desde esta librerÃ­a.\n",
    "Se debe realizar pre-procesado requerido por el modelo:\n",
    "- Tokenizar (model.tokenizer)\n",
    "- Truncar a un largo mÃ¡ximo de 512 los textos\n",
    "- Incluir tokens especiales requeridos por el modelo\n",
    "\n",
    "Luego del pre-procesado pasamos este texto tokenizado y truncado (si fuera necesario) al modelo -> Embeddings.\n",
    "\n",
    "La similitud se calcula con la similitud del coseno entre los embeddings, mÃ©todo viene incluido en la librerÃ­a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traer el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/msmarco-MiniLM-L-12-v3'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-procesado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Procesar la columna 'base_cleaned_blur'\n",
    "# tokens_1 = [model.tokenize(blurb) for blurb in book_df['base_cleaned_blur']]\n",
    "\n",
    "# # Truncar los tokens si es necesario\n",
    "# for i in range(len(tokens_1)):\n",
    "#     if len(tokens_1[i]['input_ids']) > model.max_seq_length - 2:\n",
    "#         tokens_1[i]['input_ids'] = tokens_1[i]['input_ids'][:model.max_seq_length - 2]\n",
    "#         tokens_1[i]['token_type_ids'] = tokens_1[i]['token_type_ids'][:model.max_seq_length - 2]\n",
    "#         tokens_1[i]['attention_mask'] = tokens_1[i]['attention_mask'][:model.max_seq_length - 2]\n",
    "\n",
    "# # AÃ±adir tokens especiales [CLS] y [SEP] a cada tensor\n",
    "# tokens_special_1 = [\n",
    "#     {\n",
    "#         'input_ids': [101] + tokens['input_ids'].tolist() + [102],\n",
    "#         'token_type_ids': [0] + tokens['token_type_ids'].tolist() + [0],\n",
    "#         'attention_mask': [1] + tokens['attention_mask'].tolist() + [1],\n",
    "#     }\n",
    "#     for tokens in tokens_1\n",
    "# ]\n",
    "\n",
    "# # Codificar los embeddings\n",
    "# embedding_books = model.encode(tokens_special_1, convert_to_tensor=True)\n",
    "\n",
    "# # AÃ±adir la columna de embeddings al dataframe\n",
    "# book_df['embeddings_basic_preproc'] = embedding_books.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
